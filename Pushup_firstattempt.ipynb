{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e2fffa4",
   "metadata": {},
   "source": [
    "\n",
    "# Push-up Form Checker — **All-in-One Notebook** (Prep + Train + Pose Feedback + Classifier Integration)\n",
    "\n",
    "This notebook combines:\n",
    "1. **Dataset Prep**: Reads Kaggle push-up dataset lists (`correct.txt`, `incorrect.txt`), splits by video, extracts frames to YOLO classification folders.\n",
    "2. **Classifier Training**: Trains an Ultralytics YOLO **classification** model (`correct` vs `incorrect`).\n",
    "3. **Evaluation**: Prints a classification report and saves a confusion matrix.\n",
    "4. **Rep Counter (YOLO-Pose)**: Uses YOLO **pose** to detect keypoints and count reps, with *minor feedback* (e.g., depth, hip sag, elbow flare).\n",
    "5. **Classifier Integration**: When a rep completes, grabs a few frames and runs the trained classifier to tag the rep as `correct/incorrect`.\n",
    "\n",
    "> You can run sections independently. The rep counter works with **webcam** or **video file**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df78fb30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ultralytics in c:\\users\\timle\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (8.3.203)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\timle\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (4.12.0.88)\n",
      "Requirement already satisfied: numpy in c:\\users\\timle\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.2.6)\n",
      "Requirement already satisfied: pandas in c:\\users\\timle\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\timle\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.7.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\timle\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.10.6)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\timle\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (6.0.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\timle\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: pillow>=7.1.2 in c:\\users\\timle\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from ultralytics) (11.3.0)\n",
      "Requirement already satisfied: requests>=2.23.0 in c:\\users\\timle\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from ultralytics) (2.32.5)\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\users\\timle\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from ultralytics) (1.16.2)\n",
      "Requirement already satisfied: torch>=1.8.0 in c:\\users\\timle\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from ultralytics) (2.8.0)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in c:\\users\\timle\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from ultralytics) (0.23.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\timle\\appdata\\roaming\\python\\python313\\site-packages (from ultralytics) (7.0.0)\n",
      "Requirement already satisfied: polars in c:\\users\\timle\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from ultralytics) (1.33.1)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in c:\\users\\timle\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from ultralytics) (2.0.17)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\timle\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\timle\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\timle\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\timle\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\timle\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\timle\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\timle\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\timle\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (4.60.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\timle\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\timle\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\timle\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\timle\\appdata\\roaming\\python\\python313\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\timle\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\timle\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\timle\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\timle\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\timle\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2025.8.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\timle\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\timle\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch>=1.8.0->ultralytics) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\timle\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch>=1.8.0->ultralytics) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\timle\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\timle\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\timle\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch>=1.8.0->ultralytics) (2025.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\timle\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch>=1.8.0->ultralytics) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\timle\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\timle\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === 0. Installs (uncomment if needed) =======================================\n",
    "%pip install ultralytics opencv-python numpy pandas scikit-learn matplotlib pyyaml tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d72e699",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === 1. Imports & Config =====================================================\n",
    "from ultralytics import YOLO\n",
    "import cv2, os, yaml, math, json, time, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from collections import deque, defaultdict\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------------------- USER CONFIG ------------------------------------------\n",
    "# Set your Kaggle dataset root (where videos + labels/* live):\n",
    "DATASET_DIR = \"C:/Data/hboict/Sem7-AIFS/Personal_Project\"   # <-- CHANGE THIS\n",
    "\n",
    "# Relative paths to the two label files (relative to DATASET_DIR):\n",
    "CORRECT_FILE = \"labels/correct.txt\"\n",
    "INCORRECT_FILE = \"labels/incorrect.txt\"\n",
    "\n",
    "# Where extracted frames and splits will live:\n",
    "OUTPUT_FRAMES_DIR = \"data/frames\"\n",
    "\n",
    "# Split ratios (by video), fixed seed for reproducibility:\n",
    "SPLIT = {\"train\": 0.7, \"val\": 0.15, \"test\": 0.15}\n",
    "SEED = 42\n",
    "\n",
    "# Frame extraction:\n",
    "FPS = 2            # frames per second\n",
    "IMGSZ = 224        # classifier input size\n",
    "\n",
    "# Classifier training defaults:\n",
    "CLASSIFIER_MODEL = \"yolov8n-cls.pt\"  # can try yolov8s-cls.pt later\n",
    "EPOCHS = 40\n",
    "BATCH = 64\n",
    "LR0 = 0.01\n",
    "WEIGHT_DECAY = 0.0005\n",
    "\n",
    "# Where to save YOLO runs:\n",
    "RUNS_DIR = \"runs\"\n",
    "RUN_NAME = \"pushup-cls4\"\n",
    "BEST_WEIGHTS = f\"{RUNS_DIR}/{RUN_NAME}/weights/best.pt\"\n",
    "\n",
    "\n",
    "# Pose model for rep counter:\n",
    "POSE_WEIGHTS = \"yolov8n-pose.pt\"     # small and fast; try s/m for accuracy\n",
    "CONF_THRES = 0.5\n",
    "IOU_THRES = 0.5\n",
    "\n",
    "# Rep counting & feedback:\n",
    "FRAMES_PER_REP_FOR_CLASSIFY = 7\n",
    "VOTE_STRATEGY = \"majority\"  # or \"avg_confidence\"\n",
    "USE_WEBCAM = True\n",
    "VIDEO_SOURCE = 0            # webcam index or \"/path/to/video.mp4\"\n",
    "FRAME_SKIP = 1              # process every nth frame\n",
    "SHOW_WINDOW = True\n",
    "DRAW_OVERLAY = True\n",
    "SAVE_OUTPUT = False\n",
    "OUTPUT_VIDEO = \"pushup_session_with_labels.mp4\"\n",
    "SESSION_DIR = \"sessions\"\n",
    "\n",
    "os.makedirs(SESSION_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_FRAMES_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aec150a",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Dataset Discovery + Split (by video)\n",
    "Reads the two label files (correct/incorrect), builds a dataframe, and assigns each **video** to train/val/test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c152c0df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "      <th>video</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:\\Data\\hboict\\Sem7-AIFS\\Personal_Project\\Corr...</td>\n",
       "      <td>correct</td>\n",
       "      <td>Copy of push up 1</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C:\\Data\\hboict\\Sem7-AIFS\\Personal_Project\\Corr...</td>\n",
       "      <td>correct</td>\n",
       "      <td>Copy of push up 1</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C:\\Data\\hboict\\Sem7-AIFS\\Personal_Project\\Corr...</td>\n",
       "      <td>correct</td>\n",
       "      <td>Copy of push up 100</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C:\\Data\\hboict\\Sem7-AIFS\\Personal_Project\\Corr...</td>\n",
       "      <td>correct</td>\n",
       "      <td>Copy of push up 100</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C:\\Data\\hboict\\Sem7-AIFS\\Personal_Project\\Corr...</td>\n",
       "      <td>correct</td>\n",
       "      <td>Copy of push up 101</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path    label  \\\n",
       "0  C:\\Data\\hboict\\Sem7-AIFS\\Personal_Project\\Corr...  correct   \n",
       "1  C:\\Data\\hboict\\Sem7-AIFS\\Personal_Project\\Corr...  correct   \n",
       "2  C:\\Data\\hboict\\Sem7-AIFS\\Personal_Project\\Corr...  correct   \n",
       "3  C:\\Data\\hboict\\Sem7-AIFS\\Personal_Project\\Corr...  correct   \n",
       "4  C:\\Data\\hboict\\Sem7-AIFS\\Personal_Project\\Corr...  correct   \n",
       "\n",
       "                 video  split  \n",
       "0    Copy of push up 1    val  \n",
       "1    Copy of push up 1    val  \n",
       "2  Copy of push up 100  train  \n",
       "3  Copy of push up 100  train  \n",
       "4  Copy of push up 101  train  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def read_list(p: Path):\n",
    "    with open(p, \"r\") as f:\n",
    "        return [ln.strip() for ln in f if ln.strip()]\n",
    "\n",
    "def discover_videos(dataset_dir: Path, correct_file: str, incorrect_file: str):\n",
    "    correct_videos = read_list(dataset_dir / correct_file)\n",
    "    incorrect_videos = read_list(dataset_dir / incorrect_file)\n",
    "    rows = []\n",
    "    for v in correct_videos:\n",
    "        rows.append({\"path\": str(dataset_dir / v), \"label\": \"correct\", \"video\": Path(v).stem})\n",
    "    for v in incorrect_videos:\n",
    "        rows.append({\"path\": str(dataset_dir / v), \"label\": \"incorrect\", \"video\": Path(v).stem})\n",
    "    df = pd.DataFrame(rows)\n",
    "    missing = df[~df[\"path\"].apply(lambda p: Path(p).exists())]\n",
    "    if not missing.empty:\n",
    "        print(\"WARNING: Some listed videos are missing on disk:\\n\", missing)\n",
    "    return df\n",
    "\n",
    "def split_by_video(df: pd.DataFrame, split_cfg: dict, seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    videos = df[\"video\"].unique().tolist()\n",
    "    random.shuffle(videos)\n",
    "    n = len(videos)\n",
    "    n_train = int(split_cfg[\"train\"] * n)\n",
    "    n_val = int(split_cfg[\"val\"] * n)\n",
    "    train_videos = set(videos[:n_train])\n",
    "    val_videos = set(videos[n_train:n_train+n_val])\n",
    "    test_videos = set(videos[n_train+n_val:])\n",
    "\n",
    "    def assign_split(v):\n",
    "        if v in train_videos: return \"train\"\n",
    "        if v in val_videos: return \"val\"\n",
    "        return \"test\"\n",
    "\n",
    "    df = df.copy()\n",
    "    df[\"split\"] = df[\"video\"].apply(assign_split)\n",
    "    return df\n",
    "\n",
    "dataset_dir = Path(DATASET_DIR)\n",
    "df = discover_videos(dataset_dir, CORRECT_FILE, INCORRECT_FILE)\n",
    "df = split_by_video(df, split_cfg=SPLIT, seed=SEED)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c0428d",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Extract Frames into YOLO Classification Folders\n",
    "Creates `data/frames/{train,val,test}/{correct,incorrect}` and samples frames at `FPS`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0ac0660",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting frames: 100%|██████████| 200/200 [01:27<00:00,  2.28it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data\\frames\\val\\correct\\Copy of push up 1_f000...</td>\n",
       "      <td>correct</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data\\frames\\val\\correct\\Copy of push up 1_f000...</td>\n",
       "      <td>correct</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data\\frames\\val\\correct\\Copy of push up 1_f000...</td>\n",
       "      <td>correct</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data\\frames\\val\\correct\\Copy of push up 1_f000...</td>\n",
       "      <td>correct</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data\\frames\\val\\correct\\Copy of push up 1_f000...</td>\n",
       "      <td>correct</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path    label split\n",
       "0  data\\frames\\val\\correct\\Copy of push up 1_f000...  correct   val\n",
       "1  data\\frames\\val\\correct\\Copy of push up 1_f000...  correct   val\n",
       "2  data\\frames\\val\\correct\\Copy of push up 1_f000...  correct   val\n",
       "3  data\\frames\\val\\correct\\Copy of push up 1_f000...  correct   val\n",
       "4  data\\frames\\val\\correct\\Copy of push up 1_f000...  correct   val"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def extract_frames(df: pd.DataFrame, output_dir: Path, fps: int = 2, imgsz: int = 224):\n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        for cls in [\"correct\", \"incorrect\"]:\n",
    "            (output_dir / split / cls).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    written = []\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Extracting frames\"):\n",
    "        video_path = row[\"path\"]\n",
    "        label = row[\"label\"]\n",
    "        split = row[\"split\"]\n",
    "        vid = cv2.VideoCapture(video_path)\n",
    "        if not vid.isOpened():\n",
    "            print(f\"Could not open {video_path}\")\n",
    "            continue\n",
    "        vfps = vid.get(cv2.CAP_PROP_FPS)\n",
    "        if vfps <= 0: vfps = 30.0\n",
    "        interval = max(int(round(vfps / fps)), 1)\n",
    "        frame_idx = 0\n",
    "        saved_idx = 0\n",
    "        while True:\n",
    "            ret, frame = vid.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            if frame_idx % interval == 0:\n",
    "                h, w = frame.shape[:2]\n",
    "                side = min(h, w)\n",
    "                y0 = (h - side) // 2\n",
    "                x0 = (w - side) // 2\n",
    "                crop = frame[y0:y0+side, x0:x0+side]\n",
    "                img = cv2.resize(crop, (imgsz, imgsz), interpolation=cv2.INTER_AREA)\n",
    "                out_name = f\"{Path(video_path).stem}_f{saved_idx:05d}.jpg\"\n",
    "                out_path = output_dir / split / label / out_name\n",
    "                cv2.imwrite(str(out_path), img)\n",
    "                written.append({\"path\": str(out_path), \"label\": label, \"split\": split})\n",
    "                saved_idx += 1\n",
    "            frame_idx += 1\n",
    "        vid.release()\n",
    "    return pd.DataFrame(written)\n",
    "\n",
    "frames_index = extract_frames(df, output_dir=Path(OUTPUT_FRAMES_DIR), fps=FPS, imgsz=IMGSZ)\n",
    "frames_index.to_csv(Path(OUTPUT_FRAMES_DIR)/\"frames_index.csv\", index=False)\n",
    "frames_index.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef703aa4",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Train YOLO Classification\n",
    "Train an image classifier on extracted frames. Results land in `runs/classify/pushup-cls/`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79b42b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.223 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.203  Python-3.13.7 torch-2.8.0+cpu CPU (13th Gen Intel Core i7-13700H)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=64, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=data/frames, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=40, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=224, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n-cls.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=pushup-cls4, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=runs, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=C:\\Data\\hboict\\Sem7-AIFS\\Personal_Project\\runs\\pushup-cls4, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=classify, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m C:\\Data\\hboict\\Sem7-AIFS\\Personal_Project\\data\\frames\\train... found 621 images in 2 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m C:\\Data\\hboict\\Sem7-AIFS\\Personal_Project\\data\\frames\\val... found 177 images in 2 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m C:\\Data\\hboict\\Sem7-AIFS\\Personal_Project\\data\\frames\\test... found 113 images in 2 classes  \n",
      "Overriding model.yaml nc=1000 with nc=2\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    332802  ultralytics.nn.modules.head.Classify         [256, 2]                      \n",
      "YOLOv8n-cls summary: 56 layers, 1,440,850 parameters, 1,440,850 gradients, 3.4 GFLOPs\n",
      "Transferred 156/158 items from pretrained weights\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.10.1 ms, read: 0.60.2 MB/s, size: 14.7 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Data\\hboict\\Sem7-AIFS\\Personal_Project\\data\\frames\\train... 621 images, 0 corrupt: 100% ━━━━━━━━━━━━ 621/621 256.7it/s 2.4s0.1s\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: C:\\Data\\hboict\\Sem7-AIFS\\Personal_Project\\data\\frames\\train.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.1 ms, read: 0.60.1 MB/s, size: 11.5 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Data\\hboict\\Sem7-AIFS\\Personal_Project\\data\\frames\\val... 177 images, 0 corrupt: 100% ━━━━━━━━━━━━ 177/177 236.8it/s 0.7s0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: C:\\Data\\hboict\\Sem7-AIFS\\Personal_Project\\data\\frames\\val.cache\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001667, momentum=0.9) with parameter groups 26 weight(decay=0.0), 27 weight(decay=0.0005), 27 bias(decay=0.0)\n",
      "Image sizes 224 train, 224 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mC:\\Data\\hboict\\Sem7-AIFS\\Personal_Project\\runs\\pushup-cls4\u001b[0m\n",
      "Starting training for 40 epochs...\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K       1/40         0G     0.8088         45        224: 100% ━━━━━━━━━━━━ 10/10 0.2it/s 48.9s.6ss\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 0.4it/s 4.5s10.8s\n",
      "                   all      0.497          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K       2/40         0G     0.5028         45        224: 100% ━━━━━━━━━━━━ 10/10 0.2it/s 47.5s.0ss\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 1.3it/s 1.5s3.7s\n",
      "                   all      0.571          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K       3/40         0G     0.3191         45        224: 100% ━━━━━━━━━━━━ 10/10 0.5it/s 18.6s.9ss\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 1.3it/s 1.5s3.9s\n",
      "                   all      0.571          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K       4/40         0G     0.2161         45        224: 100% ━━━━━━━━━━━━ 10/10 0.4it/s 28.1s.4ss\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 0.4it/s 4.7s10.8s\n",
      "                   all      0.559          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K       5/40         0G     0.1784         45        224: 100% ━━━━━━━━━━━━ 10/10 0.2it/s 47.8s.6ss\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 0.4it/s 4.7s10.6s\n",
      "                   all      0.638          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K       6/40         0G      0.155         45        224: 100% ━━━━━━━━━━━━ 10/10 0.2it/s 50.4s.7ss\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 0.4it/s 4.8s10.5s\n",
      "                   all      0.638          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K       7/40         0G    0.09678         45        224: 100% ━━━━━━━━━━━━ 10/10 0.2it/s 44.2s.7ss\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 0.4it/s 4.6s10.0s\n",
      "                   all      0.588          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K       8/40         0G    0.09684         45        224: 100% ━━━━━━━━━━━━ 10/10 0.2it/s 47.3s.6ss\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 0.4it/s 4.8s10.4s\n",
      "                   all      0.559          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K       9/40         0G    0.06488         45        224: 100% ━━━━━━━━━━━━ 10/10 0.2it/s 51.4s.9ss\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 0.4it/s 4.5s10.2s\n",
      "                   all      0.559          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      10/40         0G    0.08002         45        224: 100% ━━━━━━━━━━━━ 10/10 0.2it/s 52.2s.3ss\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 0.4it/s 5.2s11.7s\n",
      "                   all      0.638          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      11/40         0G    0.06361         45        224: 100% ━━━━━━━━━━━━ 10/10 0.2it/s 53.7s.2ss\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 0.4it/s 5.1s11.5s\n",
      "                   all      0.542          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      12/40         0G    0.07526         45        224: 100% ━━━━━━━━━━━━ 10/10 0.2it/s 51.7s.2ss\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 0.4it/s 5.0s11.2s\n",
      "                   all      0.605          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      13/40         0G    0.06575         45        224: 100% ━━━━━━━━━━━━ 10/10 0.2it/s 50.9s.9ss\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 0.4it/s 5.4s12.5s\n",
      "                   all      0.655          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      14/40         0G     0.0588         45        224: 100% ━━━━━━━━━━━━ 10/10 0.2it/s 50.8s.0ss\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 0.4it/s 4.7s10.5s\n",
      "                   all      0.712          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      15/40         0G     0.0882         45        224: 100% ━━━━━━━━━━━━ 10/10 0.2it/s 49.6s.8ss\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 0.4it/s 4.8s10.5s\n",
      "                   all      0.729          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      16/40         0G    0.06215         45        224: 100% ━━━━━━━━━━━━ 10/10 0.2it/s 52.8s.2ss\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 0.4it/s 5.1s12.1s\n",
      "                   all      0.638          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      17/40         0G     0.0629         45        224: 100% ━━━━━━━━━━━━ 10/10 0.2it/s 47.4s.4ss\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 0.4it/s 4.8s10.4s\n",
      "                   all      0.621          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      18/40         0G    0.04242         45        224: 100% ━━━━━━━━━━━━ 10/10 0.2it/s 51.6s.1ss\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 0.4it/s 5.4s12.1s\n",
      "                   all      0.633          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      19/40         0G    0.05784         45        224: 100% ━━━━━━━━━━━━ 10/10 0.2it/s 49.6s.9ss\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 0.4it/s 4.6s10.5s\n",
      "                   all      0.616          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      20/40         0G    0.03542         45        224: 100% ━━━━━━━━━━━━ 10/10 0.2it/s 52.0s.3ss\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 0.5it/s 4.2s9.7s\n",
      "                   all      0.672          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      21/40         0G    0.04256         45        224: 100% ━━━━━━━━━━━━ 10/10 0.2it/s 51.1s.2ss\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 0.4it/s 5.1s12.0s\n",
      "                   all      0.621          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      22/40         0G    0.04869         45        224: 100% ━━━━━━━━━━━━ 10/10 0.2it/s 49.5s.1ss\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 1.1it/s 1.9s4.7s\n",
      "                   all      0.621          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      23/40         0G    0.03884         45        224: 100% ━━━━━━━━━━━━ 10/10 0.4it/s 25.2s.0s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 0.4it/s 5.3s11.6s\n",
      "                   all      0.582          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      24/40         0G    0.03193         45        224: 100% ━━━━━━━━━━━━ 10/10 0.4it/s 23.3s.6ss\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 1.4it/s 1.4s3.3s\n",
      "                   all      0.655          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      25/40         0G    0.03772         45        224: 100% ━━━━━━━━━━━━ 10/10 0.3it/s 38.3s.6ss\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 0.4it/s 5.1s10.6s\n",
      "                   all      0.593          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      26/40         0G    0.04071         45        224: 100% ━━━━━━━━━━━━ 10/10 0.2it/s 52.0s.0ss\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 0.4it/s 5.2s11.9s\n",
      "                   all      0.605          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      27/40         0G    0.02861         45        224: 100% ━━━━━━━━━━━━ 10/10 0.2it/s 42.4s.3ss\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 0.5it/s 4.4s10.1s\n",
      "                   all      0.644          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      28/40         0G    0.02502         45        224: 100% ━━━━━━━━━━━━ 10/10 0.3it/s 37.0s.2ss\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 0.9it/s 2.2s5.1s\n",
      "                   all      0.644          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      29/40         0G    0.03753         45        224: 100% ━━━━━━━━━━━━ 10/10 0.2it/s 40.6s.8ss\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 0.4it/s 4.9s11.2s\n",
      "                   all      0.644          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      30/40         0G     0.0269         45        224: 100% ━━━━━━━━━━━━ 10/10 0.4it/s 24.8s.0ss\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 0.5it/s 4.4s9.2s\n",
      "                   all      0.678          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      31/40         0G    0.04659         45        224: 100% ━━━━━━━━━━━━ 10/10 0.2it/s 40.7s.4ss\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 0.4it/s 4.8s10.1s\n",
      "                   all      0.672          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      32/40         0G    0.03033         45        224: 100% ━━━━━━━━━━━━ 10/10 0.3it/s 30.3s.8ss\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 1.3it/s 1.6s3.9s\n",
      "                   all      0.695          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      33/40         0G    0.01956         45        224: 100% ━━━━━━━━━━━━ 10/10 0.6it/s 17.5s.8ss\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 1.1it/s 1.9s4.2s\n",
      "                   all      0.701          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      34/40         0G    0.02369         45        224: 100% ━━━━━━━━━━━━ 10/10 0.6it/s 15.7s.4s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 2.0it/s 1.0s2.4s\n",
      "                   all      0.706          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      35/40         0G    0.01949         45        224: 100% ━━━━━━━━━━━━ 10/10 0.6it/s 16.8s.7ss\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 1.0it/s 2.0s5.3s\n",
      "                   all      0.706          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      36/40         0G   0.009077         45        224: 100% ━━━━━━━━━━━━ 10/10 0.4it/s 27.7s.8ss\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 0.8it/s 2.5s5.9s\n",
      "                   all      0.706          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      37/40         0G   0.009106         45        224: 100% ━━━━━━━━━━━━ 10/10 0.4it/s 24.4s.9ss\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 1.8it/s 1.1s2.6s\n",
      "                   all      0.706          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      38/40         0G    0.01499         45        224: 100% ━━━━━━━━━━━━ 10/10 0.6it/s 17.0s.9s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 0.7it/s 3.0s7.0s\n",
      "                   all      0.706          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      39/40         0G    0.01063         45        224: 100% ━━━━━━━━━━━━ 10/10 0.3it/s 29.4s.7ss\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 1.2it/s 1.7s3.7s\n",
      "                   all      0.706          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      40/40         0G    0.01208         45        224: 100% ━━━━━━━━━━━━ 10/10 0.3it/s 32.1s.7ss\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 1.4it/s 1.4s3.2s\n",
      "                   all      0.706          1\n",
      "\n",
      "40 epochs completed in 0.487 hours.\n",
      "Optimizer stripped from C:\\Data\\hboict\\Sem7-AIFS\\Personal_Project\\runs\\pushup-cls4\\weights\\last.pt, 3.0MB\n",
      "Optimizer stripped from C:\\Data\\hboict\\Sem7-AIFS\\Personal_Project\\runs\\pushup-cls4\\weights\\best.pt, 3.0MB\n",
      "\n",
      "Validating C:\\Data\\hboict\\Sem7-AIFS\\Personal_Project\\runs\\pushup-cls4\\weights\\best.pt...\n",
      "Ultralytics 8.3.203  Python-3.13.7 torch-2.8.0+cpu CPU (13th Gen Intel Core i7-13700H)\n",
      "YOLOv8n-cls summary (fused): 30 layers, 1,437,442 parameters, 0 gradients, 3.3 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m C:\\Data\\hboict\\Sem7-AIFS\\Personal_Project\\data\\frames\\train... found 621 images in 2 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m C:\\Data\\hboict\\Sem7-AIFS\\Personal_Project\\data\\frames\\val... found 177 images in 2 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m C:\\Data\\hboict\\Sem7-AIFS\\Personal_Project\\data\\frames\\test... found 113 images in 2 classes  \n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 2/2 1.3it/s 1.5s3.6s\n",
      "                   all      0.729          1\n",
      "Speed: 0.0ms preprocess, 4.8ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1mC:\\Data\\hboict\\Sem7-AIFS\\Personal_Project\\runs\\pushup-cls4\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ultralytics.utils.metrics.ClassifyMetrics object with attributes:\n",
       "\n",
       "confusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x000002A46817C590>\n",
       "curves: []\n",
       "curves_results: []\n",
       "fitness: 0.8644067943096161\n",
       "keys: ['metrics/accuracy_top1', 'metrics/accuracy_top5']\n",
       "results_dict: {'metrics/accuracy_top1': 0.7288135886192322, 'metrics/accuracy_top5': 1.0, 'fitness': 0.8644067943096161}\n",
       "save_dir: WindowsPath('C:/Data/hboict/Sem7-AIFS/Personal_Project/runs/pushup-cls4')\n",
       "speed: {'preprocess': 0.0001960452106691855, 'inference': 4.783190960392729, 'loss': 1.1864497121107781e-05, 'postprocess': 4.971756384113414e-05}\n",
       "task: 'classify'\n",
       "top1: 0.7288135886192322\n",
       "top5: 1.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "cls_model = YOLO(CLASSIFIER_MODEL)\n",
    "results = cls_model.train(\n",
    "    data=str(OUTPUT_FRAMES_DIR),\n",
    "    epochs=EPOCHS,\n",
    "    imgsz=IMGSZ,\n",
    "    batch=BATCH,\n",
    "    lr0=LR0,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    project=RUNS_DIR,\n",
    "    name=RUN_NAME,\n",
    "    verbose=True\n",
    ")\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f78152",
   "metadata": {},
   "source": [
    "## 4.5. Test auto locate/ checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30d7b048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: C:\\Data\\hboict\\Sem7-AIFS\\Personal_Project\\data\\frames\n",
      "Removed cache: C:\\Data\\hboict\\Sem7-AIFS\\Personal_Project\\data\\frames\\train.cache\n",
      "Removed cache: C:\\Data\\hboict\\Sem7-AIFS\\Personal_Project\\data\\frames\\val.cache\n",
      "C:\\Data\\hboict\\Sem7-AIFS\\Personal_Project\\data\\frames\\train\\correct | exists=True | count=295 | samples=['C:\\\\Data\\\\hboict\\\\Sem7-AIFS\\\\Personal_Project\\\\data\\\\frames\\\\train\\\\correct\\\\Copy of push up 100_f00000.jpg', 'C:\\\\Data\\\\hboict\\\\Sem7-AIFS\\\\Personal_Project\\\\data\\\\frames\\\\train\\\\correct\\\\Copy of push up 100_f00001.jpg', 'C:\\\\Data\\\\hboict\\\\Sem7-AIFS\\\\Personal_Project\\\\data\\\\frames\\\\train\\\\correct\\\\Copy of push up 100_f00002.jpg']\n",
      "C:\\Data\\hboict\\Sem7-AIFS\\Personal_Project\\data\\frames\\train\\incorrect | exists=True | count=326 | samples=['C:\\\\Data\\\\hboict\\\\Sem7-AIFS\\\\Personal_Project\\\\data\\\\frames\\\\train\\\\incorrect\\\\10_f00000.jpg', 'C:\\\\Data\\\\hboict\\\\Sem7-AIFS\\\\Personal_Project\\\\data\\\\frames\\\\train\\\\incorrect\\\\10_f00001.jpg', 'C:\\\\Data\\\\hboict\\\\Sem7-AIFS\\\\Personal_Project\\\\data\\\\frames\\\\train\\\\incorrect\\\\10_f00002.jpg']\n",
      "C:\\Data\\hboict\\Sem7-AIFS\\Personal_Project\\data\\frames\\val\\correct | exists=True | count=38 | samples=['C:\\\\Data\\\\hboict\\\\Sem7-AIFS\\\\Personal_Project\\\\data\\\\frames\\\\val\\\\correct\\\\Copy of push up 162_f00000.jpg', 'C:\\\\Data\\\\hboict\\\\Sem7-AIFS\\\\Personal_Project\\\\data\\\\frames\\\\val\\\\correct\\\\Copy of push up 162_f00001.jpg', 'C:\\\\Data\\\\hboict\\\\Sem7-AIFS\\\\Personal_Project\\\\data\\\\frames\\\\val\\\\correct\\\\Copy of push up 162_f00002.jpg']\n",
      "C:\\Data\\hboict\\Sem7-AIFS\\Personal_Project\\data\\frames\\val\\incorrect | exists=True | count=139 | samples=['C:\\\\Data\\\\hboict\\\\Sem7-AIFS\\\\Personal_Project\\\\data\\\\frames\\\\val\\\\incorrect\\\\12_f00000.jpg', 'C:\\\\Data\\\\hboict\\\\Sem7-AIFS\\\\Personal_Project\\\\data\\\\frames\\\\val\\\\incorrect\\\\12_f00001.jpg', 'C:\\\\Data\\\\hboict\\\\Sem7-AIFS\\\\Personal_Project\\\\data\\\\frames\\\\val\\\\incorrect\\\\12_f00002.jpg']\n",
      "C:\\Data\\hboict\\Sem7-AIFS\\Personal_Project\\data\\frames\\test\\correct | exists=True | count=66 | samples=['C:\\\\Data\\\\hboict\\\\Sem7-AIFS\\\\Personal_Project\\\\data\\\\frames\\\\test\\\\correct\\\\Copy of push up 102_f00000.jpg', 'C:\\\\Data\\\\hboict\\\\Sem7-AIFS\\\\Personal_Project\\\\data\\\\frames\\\\test\\\\correct\\\\Copy of push up 102_f00001.jpg', 'C:\\\\Data\\\\hboict\\\\Sem7-AIFS\\\\Personal_Project\\\\data\\\\frames\\\\test\\\\correct\\\\Copy of push up 102_f00002.jpg']\n",
      "C:\\Data\\hboict\\Sem7-AIFS\\Personal_Project\\data\\frames\\test\\incorrect | exists=True | count=47 | samples=['C:\\\\Data\\\\hboict\\\\Sem7-AIFS\\\\Personal_Project\\\\data\\\\frames\\\\test\\\\incorrect\\\\13_f00000.jpg', 'C:\\\\Data\\\\hboict\\\\Sem7-AIFS\\\\Personal_Project\\\\data\\\\frames\\\\test\\\\incorrect\\\\13_f00001.jpg', 'C:\\\\Data\\\\hboict\\\\Sem7-AIFS\\\\Personal_Project\\\\data\\\\frames\\\\test\\\\incorrect\\\\13_f00002.jpg']\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import glob, os\n",
    "\n",
    "# ABSOLUTE paths (match your screenshot)\n",
    "ROOT_FRAMES = Path(r\"C:/Data/hboict/Sem7-AIFS/Personal_Project/data/frames\").resolve()\n",
    "TRAIN_DIR   = ROOT_FRAMES / \"train\"\n",
    "VAL_DIR     = ROOT_FRAMES / \"val\"\n",
    "TEST_DIR    = ROOT_FRAMES / \"test\"\n",
    "\n",
    "print(\"Using:\", ROOT_FRAMES)\n",
    "\n",
    "# Nuke stale Ultralytics caches so it rescans\n",
    "for f in glob.glob(str(ROOT_FRAMES / \"*.cache\")):\n",
    "    try:\n",
    "        os.remove(f); print(\"Removed cache:\", f)\n",
    "    except Exception as e:\n",
    "        print(\"Could not remove\", f, e)\n",
    "\n",
    "def count_imgs(p: Path):\n",
    "    return len(glob.glob(str(p / \"*.jpg\"))) + len(glob.glob(str(p / \"*.png\")))\n",
    "\n",
    "for split_dir in [TRAIN_DIR, VAL_DIR, TEST_DIR]:\n",
    "    for cls in [\"correct\",\"incorrect\"]:\n",
    "        d = split_dir / cls\n",
    "        samples = sorted(glob.glob(str(d / \"*.jpg\")) + glob.glob(str(d / \"*.png\")))[:3]\n",
    "        print(f\"{d} | exists={d.exists()} | count={count_imgs(d)} | samples={samples}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859cf697",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Evaluate on Test Split + Save Confusion Matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fe49c9fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.203  Python-3.13.7 torch-2.8.0+cpu CPU (13th Gen Intel Core i7-13700H)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLOv8n-cls summary (fused): 30 layers, 1,437,442 parameters, 0 gradients, 3.3 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m C:\\Data\\hboict\\Sem7-AIFS\\Personal_Project\\data\\frames\\train... found 621 images in 2 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m C:\\Data\\hboict\\Sem7-AIFS\\Personal_Project\\data\\frames\\val... found 177 images in 2 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m C:\\Data\\hboict\\Sem7-AIFS\\Personal_Project\\data\\frames\\test... found 113 images in 2 classes  \n",
      "\u001b[34m\u001b[1mtest: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 100.029.6 MB/s, size: 14.3 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtest: \u001b[0mScanning C:\\Data\\hboict\\Sem7-AIFS\\Personal_Project\\data\\frames\\test... 113 images, 0 corrupt: 100% ━━━━━━━━━━━━ 113/113 87.3Kit/s 0.0s\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 8/8 9.7it/s 0.8s0.1s\n",
      "                   all      0.867          1\n",
      "Speed: 0.0ms preprocess, 4.6ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1mC:\\Data\\hboict\\Sem7-AIFS\\Personal_Project\\runs\\classify\\val3\u001b[0m\n",
      "ultralytics.utils.metrics.ClassifyMetrics object with attributes:\n",
      "\n",
      "confusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x000002A3E1CE1F60>\n",
      "curves: []\n",
      "curves_results: []\n",
      "fitness: 0.9336283206939697\n",
      "keys: ['metrics/accuracy_top1', 'metrics/accuracy_top5']\n",
      "results_dict: {'metrics/accuracy_top1': 0.8672566413879395, 'metrics/accuracy_top5': 1.0, 'fitness': 0.9336283206939697}\n",
      "save_dir: WindowsPath('C:/Data/hboict/Sem7-AIFS/Personal_Project/runs/classify/val3')\n",
      "speed: {'preprocess': 0.001213274173957782, 'inference': 4.572747787769751, 'loss': 7.168131785681553e-05, 'postprocess': 0.0002539824319393498}\n",
      "task: 'classify'\n",
      "top1: 0.8672566413879395\n",
      "top5: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     correct     1.0000    0.7727    0.8718        66\n",
      "   incorrect     0.7581    1.0000    0.8624        47\n",
      "\n",
      "    accuracy                         0.8673       113\n",
      "   macro avg     0.8790    0.8864    0.8671       113\n",
      "weighted avg     0.8994    0.8673    0.8679       113\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "best_weights = Path(BEST_WEIGHTS)\n",
    "if not best_weights.exists():\n",
    "    raise FileNotFoundError(f\"Best weights not found at {best_weights}. Train first.\")\n",
    "\n",
    "from ultralytics import YOLO\n",
    "eval_model = YOLO(str(best_weights))\n",
    "metrics = eval_model.val(data=str(OUTPUT_FRAMES_DIR), imgsz=IMGSZ, split=\"test\")\n",
    "print(metrics)\n",
    "\n",
    "\n",
    "# Build confusion matrix via direct predictions\n",
    "class_names = [\"correct\", \"incorrect\"]\n",
    "y_true, y_pred = [], []\n",
    "test_dir = Path(OUTPUT_FRAMES_DIR) / \"test\"\n",
    "img_paths = []\n",
    "for cls_idx, cls in enumerate(class_names):\n",
    "    for p in (test_dir / cls).glob(\"*.jpg\"):\n",
    "        img_paths.append((str(p), cls_idx))\n",
    "\n",
    "bs = 64\n",
    "for i in range(0, len(img_paths), bs):\n",
    "    batch = img_paths[i:i+bs]\n",
    "    imgs = [b[0] for b in batch]\n",
    "    gts = [b[1] for b in batch]\n",
    "    preds = eval_model(imgs, verbose=False)\n",
    "    for gt, pred in zip(gts, preds):\n",
    "        pred_cls = int(np.argmax(pred.probs.data))\n",
    "        y_true.append(gt)\n",
    "        y_pred.append(pred_cls)\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred, labels=[0,1])\n",
    "report = classification_report(y_true, y_pred, target_names=class_names, digits=4)\n",
    "print(report)\n",
    "\n",
    "def plot_confusion_matrix(cm, class_names, out_path: Path):\n",
    "    fig = plt.figure(figsize=(5,4))\n",
    "    plt.imshow(cm, interpolation='nearest')\n",
    "    plt.title('Confusion matrix')\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(class_names))\n",
    "    plt.xticks(tick_marks, class_names, rotation=45)\n",
    "    plt.yticks(tick_marks, class_names)\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, f\"{cm[i, j]}\\n({cm_normalized[i, j]:.2f})\",\n",
    "                     horizontalalignment=\"center\", verticalalignment=\"center\")\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(Path(RUNS_DIR)/RUN_NAME/\"confusion_matrix.png\", dpi=200)\n",
    "    plt.close(fig)\n",
    "\n",
    "plot_confusion_matrix(cm, class_names, Path(RUNS_DIR) / RUN_NAME / \"confusion_matrix.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858e0f63",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Rep Counter with **YOLO-Pose** + Minor Feedback\n",
    "- Uses `yolov8n-pose.pt` to extract COCO keypoints.\n",
    "- Simple heuristics compute:\n",
    "  - **Depth** (elbow angle at bottom)\n",
    "  - **Hip Sag** (shoulder–hip alignment)\n",
    "  - **Elbow Flaring** (upper-arm angle relative to torso)\n",
    "  - **Tempo** (rep duration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5610e889",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Geometry helpers --------------------------------------------------------\n",
    "def angle(a, b, c):\n",
    "    # angle at b: a-b-c\n",
    "    a, b, c = np.array(a), np.array(b), np.array(c)\n",
    "    ba = a - b\n",
    "    bc = c - b\n",
    "    cosang = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc) + 1e-6)\n",
    "    cosang = np.clip(cosang, -1.0, 1.0)\n",
    "    return math.degrees(math.acos(cosang))\n",
    "\n",
    "def line_point_distance(p, a, b):\n",
    "    # distance from p to line through a-b\n",
    "    p, a, b = np.array(p), np.array(a), np.array(b)\n",
    "    if np.allclose(a,b):\n",
    "        return np.linalg.norm(p-a)\n",
    "    return np.linalg.norm(np.cross(b-a, a-p)) / (np.linalg.norm(b-a) + 1e-6)\n",
    "\n",
    "# --- COCO keypoint indices (YOLOv8-pose) ------------------------------------\n",
    "# 0 nose, 5 left_shoulder, 6 right_shoulder, 7 left_elbow, 8 right_elbow,\n",
    "# 9 left_wrist, 10 right_wrist, 11 left_hip, 12 right_hip, 13 left_knee, 14 right_knee,\n",
    "# 15 left_ankle, 16 right_ankle\n",
    "KS = dict(nose=0, ls=5, rs=6, le=7, re=8, lw=9, rw=10, lh=11, rh=12, lk=13, rk=14, la=15, ra=16)\n",
    "\n",
    "def get_xy(kps, i):\n",
    "    # kps shape: (17,3) -> x,y,conf\n",
    "    return (float(kps[i,0]), float(kps[i,1]))\n",
    "\n",
    "def valid_triplet(kps, a,b,c, minconf=0.35):\n",
    "    return kps[a,2]>minconf and kps[b,2]>minconf and kps[c,2]>minconf\n",
    "\n",
    "# --- Form rules (heuristics) -------------------------------------------------\n",
    "def assess_form(kps, phase=\"bottom\"):\n",
    "    issues = []\n",
    "    # Depth: elbow angle at bottom should be small (e.g., < 80-90 deg)\n",
    "    if valid_triplet(kps, KS['ls'], KS['le'], KS['lw']):\n",
    "        left_elbow = angle(get_xy(kps, KS['ls']), get_xy(kps, KS['le']), get_xy(kps, KS['lw']))\n",
    "    else:\n",
    "        left_elbow = None\n",
    "    if valid_triplet(kps, KS['rs'], KS['re'], KS['rw']):\n",
    "        right_elbow = angle(get_xy(kps, KS['rs']), get_xy(kps, KS['re']), get_xy(kps, KS['rw']))\n",
    "    else:\n",
    "        right_elbow = None\n",
    "\n",
    "    elbow_min = None\n",
    "    if left_elbow is not None and right_elbow is not None:\n",
    "        elbow_min = min(left_elbow, right_elbow)\n",
    "    elif left_elbow is not None:\n",
    "        elbow_min = left_elbow\n",
    "    elif right_elbow is not None:\n",
    "        elbow_min = right_elbow\n",
    "\n",
    "    # if phase == \"bottom\" and elbow_min is not None and elbow_min > 100:\n",
    "    #     issues.append(\"Depth too shallow (elbows not bent enough)\")\n",
    "        \n",
    "    if phase == \"bottom\" and elbow_min is not None and elbow_min > 120:\n",
    "        issues.append(\"Depth too shallow (aim for elbows ~90°)\")\n",
    "\n",
    "    # Hip sag: distance of hip from line shoulder-ankle should be small\n",
    "    # Use right side if available else left\n",
    "    side = \"r\" if kps[KS['rs'],2] > kps[KS['ls'],2] else \"l\"\n",
    "    if side == \"r\":\n",
    "        parts = ['rs','rh','ra']\n",
    "    else:\n",
    "        parts = ['ls','lh','la']\n",
    "    if kps[KS[parts[0]],2]>0.35 and kps[KS[parts[1]],2]>0.35 and kps[KS[parts[2]],2]>0.35:\n",
    "        shoulder = get_xy(kps, KS[parts[0]])\n",
    "        hip = get_xy(kps, KS[parts[1]])\n",
    "        ankle = get_xy(kps, KS[parts[2]])\n",
    "        sag = line_point_distance(hip, shoulder, ankle)\n",
    "        # Normalize by body length shoulder-ankle\n",
    "        norm = np.linalg.norm(np.array(shoulder) - np.array(ankle)) + 1e-6\n",
    "        if sag / norm > 0.12:\n",
    "            issues.append(\"Hips sagging (keep core tight)\")\n",
    "\n",
    "    # Elbow flaring: angle between upper-arm and torso too large\n",
    "    # torso vector: shoulder-hip; upper-arm: shoulder-elbow\n",
    "    if kps[KS['rs'],2]>0.35 and kps[KS['rh'],2]>0.35 and kps[KS['re'],2]>0.35:\n",
    "        rs, rh, re = get_xy(kps, KS['rs']), get_xy(kps, KS['rh']), get_xy(kps, KS['re'])\n",
    "        torso_vec = np.array(rh) - np.array(rs)\n",
    "        arm_vec = np.array(re) - np.array(rs)\n",
    "        cosang = abs(np.dot(torso_vec, arm_vec)) / (np.linalg.norm(torso_vec)*np.linalg.norm(arm_vec)+1e-6)\n",
    "        flare_angle = math.degrees(math.acos(np.clip(cosang, -1, 1)))\n",
    "        if flare_angle > 65:  # heuristic threshold\n",
    "            issues.append(\"Elbows flaring out (keep ~45° from torso)\")\n",
    "\n",
    "    return issues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4cc157af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Pose-based rep counter --------------------------------------------------\n",
    "class PoseRepCounter:\n",
    "    def __init__(self, pose_model, frames_for_classify=7, vote_strategy=\"majority\"):\n",
    "        self.model = pose_model\n",
    "        self.prev_y = deque(maxlen=5)\n",
    "        self.state = \"up\"\n",
    "        self.last_bottom_time = None\n",
    "        self.reps = 0\n",
    "        self.rep_events = []  # list of dicts\n",
    "        self.frames_buffer = deque(maxlen=frames_for_classify)\n",
    "        self.frames_for_classify = frames_for_classify\n",
    "        self.vote_strategy = vote_strategy\n",
    "\n",
    "    def update(self, frame):\n",
    "        self.frames_buffer.append(frame.copy())\n",
    "        res = self.model(frame, verbose=False)[0]\n",
    "        phase = None\n",
    "        kps_best = None\n",
    "\n",
    "        # choose highest-conf person\n",
    "        if res.keypoints is not None and len(res.keypoints) > 0:\n",
    "            # Each item has .xy (N,17,2) and .conf (N,17)\n",
    "            best_i = 0\n",
    "            best_score = -1\n",
    "            for i, kp in enumerate(res.keypoints):\n",
    "                conf = float(kp.conf.mean())\n",
    "                if conf > best_score:\n",
    "                    best_score = conf\n",
    "                    best_i = i\n",
    "            kps = res.keypoints[best_i].data[0].cpu().numpy()  # (17,3)\n",
    "            kps_best = kps\n",
    "\n",
    "            # vertical proxy: average y of shoulders & hips\n",
    "            y_points = []\n",
    "            for idx in [KS['ls'], KS['rs'], KS['lh'], KS['rh']]:\n",
    "                if kps[idx,2] > 0.35:\n",
    "                    y_points.append(kps[idx,1])\n",
    "            if len(y_points)>=2:\n",
    "                y_mean = float(np.mean(y_points))\n",
    "                self.prev_y.append(y_mean)\n",
    "\n",
    "            if len(self.prev_y) == self.prev_y.maxlen:\n",
    "                dy = self.prev_y[-1] - self.prev_y[0]\n",
    "                # simple hysteresis on y to detect down/up\n",
    "                if self.state == \"up\" and dy > 8:\n",
    "                    self.state = \"down\"\n",
    "                    phase = \"down\"\n",
    "                elif self.state == \"down\" and dy < -8:\n",
    "                    self.state = \"up\"\n",
    "                    phase = \"up\"\n",
    "                    # Up transition after down => count a rep\n",
    "                    self.reps += 1\n",
    "                    issues = assess_form(kps, phase=\"bottom\")\n",
    "                    event = {\n",
    "                        \"rep\": self.reps,\n",
    "                        \"time\": time.time(),\n",
    "                        \"issues\": issues,\n",
    "                        \"kps_conf\": float(np.mean(kps[:,2]))\n",
    "                    }\n",
    "                    self.rep_events.append(event)\n",
    "                    return True, event, kps_best\n",
    "\n",
    "        return False, None, kps_best\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885fcd7b",
   "metadata": {},
   "source": [
    "# 6.5 or something:\n",
    "Check for accurate labeling of frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b079f4ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy of push up 102_f00003.jpg {0: 'correct', 1: 'incorrect'} 1 incorrect\n",
      "Copy of push up 107_f00001.jpg {0: 'correct', 1: 'incorrect'} 1 incorrect\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "from pathlib import Path\n",
    "import random\n",
    "import cv2\n",
    "\n",
    "model = YOLO(\"runs/pushup-cls4/weights/best.pt\")\n",
    "\n",
    "# Get one random frame from each class\n",
    "correct_sample = random.choice(list(Path(\"data/frames/test/correct\").glob(\"*.jpg\")))\n",
    "incorrect_sample = random.choice(list(Path(\"data/frames/test/incorrect\").glob(\"*.jpg\")))\n",
    "\n",
    "for img_path in [correct_sample, incorrect_sample]:\n",
    "    result = model(img_path, verbose=False)[0]\n",
    "    print(img_path.name, result.names, result.probs.top1, result.names[result.probs.top1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca6c4de",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Integrate Classifier into Pose Rep Counter\n",
    "When a rep is detected, we classify the buffered frames as `correct/incorrect` using the trained classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3718d4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting loop. Press 'q' to quit.\n",
      "[REP 1] issues=[] label=incorrect (0.999)\n",
      "[REP 2] issues=[] label=incorrect (0.972)\n",
      "[REP 3] issues=[] label=incorrect (0.8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\timle\\AppData\\Local\\Temp\\ipykernel_25532\\2982038869.py:16: DeprecationWarning: Arrays of 2-dimensional vectors are deprecated. Use arrays of 3-dimensional vectors instead. (deprecated in NumPy 2.0)\n",
      "  return np.linalg.norm(np.cross(b-a, a-p)) / (np.linalg.norm(b-a) + 1e-6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REP 4] issues=['Depth too shallow (elbows not bent enough)', 'Hips sagging (keep core tight)', 'Elbows flaring out (keep ~45° from torso)'] label=incorrect (0.981)\n",
      "[REP 5] issues=['Depth too shallow (elbows not bent enough)', 'Elbows flaring out (keep ~45° from torso)'] label=incorrect (0.999)\n",
      "[REP 6] issues=['Depth too shallow (elbows not bent enough)', 'Elbows flaring out (keep ~45° from torso)'] label=incorrect (0.999)\n",
      "[REP 7] issues=['Depth too shallow (elbows not bent enough)'] label=incorrect (0.997)\n",
      "[REP 8] issues=['Depth too shallow (elbows not bent enough)'] label=incorrect (0.998)\n",
      "[REP 9] issues=[] label=incorrect (0.996)\n",
      "[REP 10] issues=['Depth too shallow (elbows not bent enough)', 'Elbows flaring out (keep ~45° from torso)'] label=incorrect (0.996)\n",
      "[REP 11] issues=['Depth too shallow (elbows not bent enough)', 'Elbows flaring out (keep ~45° from torso)'] label=incorrect (0.997)\n",
      "[REP 12] issues=['Depth too shallow (elbows not bent enough)'] label=incorrect (0.997)\n",
      "[REP 13] issues=['Depth too shallow (elbows not bent enough)', 'Hips sagging (keep core tight)', 'Elbows flaring out (keep ~45° from torso)'] label=incorrect (0.991)\n",
      "[REP 14] issues=['Depth too shallow (elbows not bent enough)', 'Hips sagging (keep core tight)', 'Elbows flaring out (keep ~45° from torso)'] label=incorrect (0.986)\n",
      "[REP 15] issues=[] label=incorrect (0.936)\n",
      "[REP 16] issues=['Depth too shallow (elbows not bent enough)', 'Elbows flaring out (keep ~45° from torso)'] label=incorrect (0.892)\n",
      "[REP 17] issues=[] label=incorrect (0.925)\n",
      "[REP 18] issues=['Depth too shallow (elbows not bent enough)'] label=incorrect (0.965)\n",
      "[REP 19] issues=[] label=incorrect (0.949)\n",
      "[REP 20] issues=['Depth too shallow (elbows not bent enough)', 'Elbows flaring out (keep ~45° from torso)'] label=incorrect (0.965)\n",
      "[REP 21] issues=[] label=incorrect (0.961)\n",
      "[REP 22] issues=[] label=incorrect (0.882)\n",
      "[REP 23] issues=[] label=incorrect (0.996)\n",
      "[REP 24] issues=[] label=incorrect (0.967)\n",
      "[REP 25] issues=[] label=incorrect (0.999)\n",
      "Saved session log to sessions\\session_1762010440.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load pose + classifier\n",
    "pose_model = YOLO(POSE_WEIGHTS)\n",
    "classifier = YOLO(str(Path(BEST_WEIGHTS))) if Path(BEST_WEIGHTS).exists() else None\n",
    "\n",
    "# Voting helper\n",
    "def vote_frames(model, frames, strategy=\"majority\"):\n",
    "    preds = []\n",
    "    for f in frames:\n",
    "        r = model(f, verbose=False)[0]\n",
    "        top1 = int(r.probs.top1); conf = float(r.probs.top1conf)\n",
    "        preds.append((top1, conf))\n",
    "    counts = defaultdict(int); confs = defaultdict(list)\n",
    "    for cls_idx, c in preds:\n",
    "        counts[cls_idx]+=1; confs[cls_idx].append(c)\n",
    "    if strategy == \"avg_confidence\":\n",
    "        best_cls = max(confs.keys(), key=lambda c: np.mean(confs[c]))\n",
    "        best_conf = float(np.mean(confs[best_cls]))\n",
    "    else:\n",
    "        max_votes = max(counts.values())\n",
    "        majority = [c for c,v in counts.items() if v==max_votes]\n",
    "        if len(majority)==1:\n",
    "            best_cls = majority[0]; best_conf = float(np.mean(confs[best_cls]))\n",
    "        else:\n",
    "            best_cls = max(majority, key=lambda c: np.mean(confs[c]))\n",
    "            best_conf = float(np.mean(confs[best_cls]))\n",
    "    return best_cls, best_conf\n",
    "\n",
    "counter = PoseRepCounter(pose_model, frames_for_classify=FRAMES_PER_REP_FOR_CLASSIFY, vote_strategy=VOTE_STRATEGY)\n",
    "\n",
    "# Live/video loop\n",
    "if USE_WEBCAM:\n",
    "    cap = cv2.VideoCapture(VIDEO_SOURCE)\n",
    "else:\n",
    "    cap = cv2.VideoCapture(VIDEO_SOURCE)\n",
    "\n",
    "if SAVE_OUTPUT:\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    out = cv2.VideoWriter(OUTPUT_VIDEO, fourcc, 30.0, (int(cap.get(3)), int(cap.get(4))))\n",
    "else:\n",
    "    out = None\n",
    "\n",
    "rep_log = []\n",
    "last_rep_info = None\n",
    "frame_idx = 0\n",
    "print(\"Starting loop. Press 'q' to quit.\")\n",
    "\n",
    "while True:\n",
    "    ok, frame = cap.read()\n",
    "    if not ok:\n",
    "        break\n",
    "    frame_idx += 1\n",
    "    if frame_idx % FRAME_SKIP != 0:\n",
    "        if SHOW_WINDOW and DRAW_OVERLAY:\n",
    "            cv2.imshow(\"Pose Rep + Feedback + Classifier\", frame)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "        continue\n",
    "\n",
    "    rep_event, event, kps_best = counter.update(frame)\n",
    "\n",
    "    if rep_event:\n",
    "        label_txt, conf_txt = None, None\n",
    "        if classifier is not None and len(counter.frames_buffer) >= FRAMES_PER_REP_FOR_CLASSIFY:\n",
    "            frames_for_rep = list(counter.frames_buffer)[-FRAMES_PER_REP_FOR_CLASSIFY:]\n",
    "            best_cls, best_conf = vote_frames(classifier, frames_for_rep, strategy=VOTE_STRATEGY)\n",
    "            label_txt = classifier.names[best_cls]\n",
    "            conf_txt = round(best_conf, 3)\n",
    "\n",
    "        info = {\n",
    "            \"rep\": event[\"rep\"],\n",
    "            \"timestamp\": time.time(),\n",
    "            \"issues\": event[\"issues\"],\n",
    "            \"kps_conf\": event[\"kps_conf\"],\n",
    "            \"label\": label_txt,\n",
    "            \"label_conf\": conf_txt\n",
    "        }\n",
    "        rep_log.append(info)\n",
    "        last_rep_info = info\n",
    "        print(f\"[REP {info['rep']}] issues={info['issues']} label={label_txt} ({conf_txt})\")\n",
    "\n",
    "    # Overlay\n",
    "    if DRAW_OVERLAY:\n",
    "        vis = frame.copy()\n",
    "        cv2.putText(vis, f\"Reps: {counter.reps}\", (20,40), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255,255,255), 2, cv2.LINE_AA)\n",
    "        if last_rep_info:\n",
    "            last = last_rep_info\n",
    "            msg = f\"Last: {last.get('label','?')} ({last.get('label_conf','-')})\"\n",
    "            cv2.putText(vis, msg, (20,80), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255,255,255), 2, cv2.LINE_AA)\n",
    "            if len(last['issues'])>0:\n",
    "                cv2.putText(vis, \"Issues: \" + \"; \".join(last['issues'][:2]), (20,120),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255,255,255), 2, cv2.LINE_AA)\n",
    "        frame = vis\n",
    "\n",
    "    if out is not None:\n",
    "        out.write(frame)\n",
    "\n",
    "    if SHOW_WINDOW:\n",
    "        cv2.imshow(\"Pose Rep + Feedback + Classifier\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "if out is not None:\n",
    "    out.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Save session\n",
    "os.makedirs(SESSION_DIR, exist_ok=True)\n",
    "log_path = Path(SESSION_DIR)/f\"session_{int(time.time())}.json\"\n",
    "with open(log_path, \"w\") as f:\n",
    "    json.dump(rep_log, f, indent=2)\n",
    "print(\"Saved session log to\", str(log_path))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
